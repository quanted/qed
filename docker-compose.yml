version: '2.1'
volumes:
  collected_static:
services:

  # QED Django front-end
  qed_django:
    restart: unless-stopped
    image: dbsmith88/qed_django  # use qed image w/ tag = VERSION, default = latest
    expose:
      - "8080"
    volumes:
      - collected_static:/src/collected_static
      - .:/src  # map qed/ to container's /src for updating w/out rebuilding images
      - /var/www/app_data:/src/app-data
    environment:
      - REDIS_HOSTNAME=redis
      #these echo in the environmental variable to the running docker container
      #so that is can be picked up by the django settings 
      - DOCKER_HOSTNAME=${HOSTNAME}
      #- DOCKER_SECRET_KEY=${SECRET_KEY}
      - HMS_RELEASE=${HMS_RELASE:-0}
      - IN_PROD=${IN_PROD:-0}
    links:
      - redis
      - mongodb

  # Redis (message broker)
  redis:
    restart: unless-stopped
    image: redis:latest
    hostname: redis
    command: redis-server /usr/local/etc/redis/redis.conf
    volumes:
      - ./redis.config:/usr/local/etc/redis/redis.conf
    expose:
      - "6379"

  # ubertool_cts nodejs submodule
  cts_nodejs:
    restart: unless-stopped
    build: ./cts_nodejs
    # image: cts-nodejs
    image: quanted/cts_nodejs
    expose:
      - "4000"
    environment:
      - NODEJS_HOST=cts_nodejs
      - NODEJS_PORT=4000
      - REDIS_HOSTNAME=redis
      - REDIS_PORT=6379
    links:
      - redis
    # volumes:
    #   - ./cts_nodejs:/src

  # # Celery worker - manager calc
  worker_manager:
    restart: unless-stopped
    build:
      context: ./cts_celery
      dockerfile: Dockerfile
    # image: cts-celery
    image: quanted/cts_celery
    # command: celery worker -A tasks -Q manager_queue -l info -n manager_worker -c 1
    command: celery -A tasks worker -Q manager_queue -l info -n manager_worker -c 1
    links:
      - redis
    environment:
      - REDIS_HOSTNAME=redis
      - DOCKER_HOSTNAME=${HOSTNAME}
    volumes:
      - ./cts_celery:/src

  # # Celery worker - cts calc
  worker_cts:
    restart: unless-stopped
    build:
      context: ./cts_celery
      dockerfile: Dockerfile
    # image: cts-celery
    image: quanted/cts_celery
    # command: celery worker -A tasks -Q cts_queue -l info -n cts_worker -c 2
    command: celery -A tasks worker -Q cts_queue -l info -n cts_worker -c 2
    links:
      - redis
      - qed_django
    environment:
      - REDIS_HOSTNAME=redis
      - DOCKER_HOSTNAME=${HOSTNAME}
    volumes:
      - ./cts_celery:/src

  qed_nginx:
    restart: unless-stopped
    # build: ../cts_nginx
    build: ./qed_nginx
    ports:
      - "80:80"
      - "443:443"
    links:
      - qed_django:uwsgi_django  # Nginx.conf can reference "qed_django" service with the hostname 'uwsgi' or 'qed_django'
      - cts_nodejs:cts_nodejs
      - qed_flask:qed_flask
      - cyan-api:uwsgi_flask
    volumes:
      - /var/www/nginx/certs:/etc/nginx/qed # this points to the keys directory
      - /etc/letsencrypt:/etc/letsencrypt  # certs from letsencrypt (uses certbot)
    volumes_from:
      - qed_django:ro  # Mount all volumes from "qed_django" to NGINX, so it can access the collected static files

  # flask_qed Flask back-end
  qed_flask:
    restart: unless-stopped
    image: dbsmith88/flask_qed  # use qed image w/ tag = VERSION, default = latest
    expose:
      - "7777"
    links:
      - redis
      - mongodb
      - qed_celery
    environment:
      - REDIS_HOSTNAME=redis
      - DOCKER_HOSTNAME=${HOSTNAME}
    volumes:
      - ./flask_qed:/src
      - /var/www/sampreprocessed:/src/pram_flask/ubertool/ubertool/sam/bin/Preprocessed
      - /var/www/samresults:/src/pram_flask/ubertool/ubertool/sam/bin/Results
      - /var/www/qed-basins:/src/hms_flask/data/qed-basins
      - collected_static:/src/collected_static
      - /var/www/app_data/hms:/src/hms-data
    depends_on:
      - mongodb
    logging:
      options:
        max-size: "200k"
        max-file: "10"

  # mongoDB database container
  mongodb:
    restart: unless-stopped
    image: mongo:latest
    volumes:
      - /var/www/mongodb:/data/db
    expose:
      - "27017"

  # Celery container for async task execution
  qed_celery:
    restart: unless-stopped
    image: dbsmith88/flask_qed
    volumes:
      - ./flask_qed:/src
      - /var/www/sampreprocessed:/src/pram_flask/ubertool/ubertool/sam/bin/Preprocessed
      - "../samresults:/src/pram_flask/ubertool/ubertool/sam/bin/Results"
      - "../qed-basins:/src/hms_flask/data/qed-basins"
      - collected_static:/src/collected_static
    links:
      - redis
      - mongodb
    command: celery -A celery_cgi worker -Q qed --loglevel=INFO -c 2 -n qed_worker
    environment:
      - REDIS_HOSTNAME=redis
      - DOCKER_HOSTNAME=${HOSTNAME}
      - DASK_SCHEDULER=dask_scheduler:8786
    logging:
      options:
        max-size: "200k"
        max-file: "10"

  hms_dotnetcore:
    restart: unless-stopped
    image: dbsmith88/hms_dotnetcore
    environment:
      - MONGODB=mongodb
      - FLASK_SERVER=http://qed_nginx:7777
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
    expose:
     - "80"
    volumes:
      #- "./windows/hms/Web.Services/App_Data:/app/App_Data"
      - /var/www/app_data/hms:/app/App_Data

  # Dask Containers
  dask_scheduler:
    restart: unless-stopped
    image: dbsmith88/qed-dask
    hostname: dask-scheduler
    expose:
     - 8786
     - 8787
    command: ["dask-scheduler"]

  dask_worker:
    restart: unless-stopped
    image: dbsmith88/qed-dask
    hostname: dask-worker
    environment:
      PYTHONPATH: '/src:/src/qed:/src/qed/flask_qed'
      IN_DOCKER: "True"
      HMS_BACKEND_SERVER_INTERNAL: "http://hms_dotnetcore:80/"
    volumes:
       - "../qed:/src/qed"

  # Apache Tomcat container
  qed-tomcat:
    restart: unless-stopped
    build:
      context: .
      dockerfile: Dockerfile_Tomcat
    image: quanted/qed-tomcat
    expose:
      - "8080"
    environment:
      - JAVA_OPTS=-Xmx1g
    volumes:
      - ./secrets/tomcat/tomcat-users.xml:/usr/local/tomcat/conf/tomcat-users.xml
      - ./secrets/tomcat/webapps:/usr/local/tomcat/webapps
      - ./secrets/tomcat/chemaxon/licenses:/home/tomcat/.chemaxon/licenses

  pisces-db:
    restart: unless-stopped
    image: mdillon/postgis
    expose:
      - "5432"
    volumes:
      - /var/www/pisces:/docker-entrypoint-initdb.d

  # Cyanweb Flask API
  cyan-api:
    restart: unless-stopped
    build:
      context: ./EPA-Cyano-Web
      dockerfile: docker/flask/Dockerfile
    expose:
      - "5001"
    depends_on:
      - cyan-db
    volumes:
      - ./EPA-Cyano-Web/config:/config
      - ./EPA-Cyano-Web/cyan_flask/:/cyan_flask
    environment:
      # Does environment overwrite env_file, or vice versa?
      - DOCKER_HOSTNAME=${HOSTNAME}
    links:
      - cyan-db
    env_file:
      # Would set_enviornment.py overwrite env vars in default below (hope so).
      - ${CYAN_CONFIG:-./EPA-Cyano-Web/config/.env}

  # Cyanweb Celery Worker
  cyan-celery:
    restart: unless-stopped
    build:
      context: ./EPA-Cyano-Web
      dockerfile: docker/flask/Dockerfile
    command: celery -A celery_worker.celery worker --loglevel=INFO -c 1
    depends_on:
      - cyan-db
      - redis
    volumes:
      - ./EPA-Cyano-Web/config:/config
      - ./EPA-Cyano-Web/cyan_flask/:/cyan_flask
    environment:
      - DOCKER_HOSTNAME=${HOSTNAME}
    env_file:
      - ${CYAN_CONFIG:-./EPA-Cyano-Web/config/.env}

  # Cyanweb MySQL DB
  cyan-db:
    restart: unless-stopped
    build:
      context: ./EPA-Cyano-Web
      dockerfile: docker/mysql/Dockerfile
    volumes:
      # NOTE: Optional volume for existing DB dump, defaults to blank dummy file if no dump
      - ${SQL_DUMP:-./EPA-Cyano-Web/cyan_mysql/no_dump.txt}:/tmp/dump.sql  # use optional dump file to build container DB
      - /var/www/app_data/cyano:/var/lib/mysql
    environment:
      - MYSQL_RANDOM_ROOT_PASSWORD=yes
    env_file:
      - ${CYAN_CONFIG:-./EPA-Cyano-Web/config/.env}
